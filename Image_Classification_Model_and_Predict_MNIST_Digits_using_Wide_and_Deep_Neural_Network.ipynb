{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLsFCtNKXzOsPTvefBtAW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KMMohiuddin/DATA_SCIENCE_Projects/blob/main/Image_Classification_Model_and_Predict_MNIST_Digits_using_Wide_and_Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DS424 Lab Assignments [MHS_Spring_23] \n",
        "##### Name: K M Mohiuddin\n",
        "######  ID: 192-35-2894\n",
        "\n"
      ],
      "metadata": {
        "id": "vpFa4OHM6wMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "游릭 Question 1\n",
        "```"
      ],
      "metadata": {
        "id": "BwGP2nD77N6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build an image classification model and predict MNIST Digits using Wide and Deep Neural Network. Your should use callback functions to implement early stopping and save the best model into appropriate format. Report the training and test accuracy and other evaluation metrics.** "
      ],
      "metadata": {
        "id": "WZ9HoE85LtT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ec544QKd6CkK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scirkoso7u7g",
        "outputId": "7d1c357e-e520-4368-9621-36d59ec65fe6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the architecture of the Wide and Deep Neural Network."
      ],
      "metadata": {
        "id": "GajEJ4eS91K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input layer\n",
        "input_layer = Input(shape=(28, 28))\n",
        "\n",
        "# Flatten the input\n",
        "flatten_layer = keras.layers.Flatten()(input_layer)\n",
        "\n",
        "# Define the wide branch\n",
        "wide_branch = Dense(128, activation='relu')(flatten_layer)\n",
        "wide_branch = Dense(64, activation='relu')(wide_branch)\n",
        "\n",
        "# Define the deep branch\n",
        "deep_branch = Dense(128, activation='relu')(flatten_layer)\n",
        "deep_branch = Dense(128, activation='relu')(deep_branch)\n",
        "deep_branch = Dense(64, activation='relu')(deep_branch)\n",
        "\n",
        "# Concatenate the wide and deep branches\n",
        "concat_layer = Concatenate()([wide_branch, deep_branch])\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(10, activation='softmax')(concat_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n"
      ],
      "metadata": {
        "id": "2QOg7pu98Omf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting up early stopping and model checkpoint callbacks to monitor the training and save the best model."
      ],
      "metadata": {
        "id": "q4bqK95z-IeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "LUcF7pHh-Cmw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n"
      ],
      "metadata": {
        "id": "7L7DfWJb-TrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model saved during training\n",
        "best_model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test Accuracy:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L59cOya-Zvv",
        "outputId": "e9d29323-a72e-4a11-e72b-2e4b17ee9778"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0860 - accuracy: 0.9734\n",
            "Test Accuracy: 0.9733999967575073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test data\n",
        "predictions = best_model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "true_labels = tf.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = tf.keras.metrics.Precision()(true_labels, predicted_labels)\n",
        "recall = tf.keras.metrics.Recall()(true_labels, predicted_labels)\n",
        "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Precision:', precision.numpy())\n",
        "print('Recall:', recall.numpy())\n",
        "print('F1 Score:', f1_score.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DtquIxt_Pso",
        "outputId": "c00b309a-b76a-4afc-9b5a-c8b4e72a04b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "Precision: 0.9991111\n",
            "Recall: 0.9968958\n",
            "F1 Score: 0.9980022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "游릭 Question 2:\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QLt0nhKw_lgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This time we used diffrent activation function like teanh, sigmoid insted of relu used before.**"
      ],
      "metadata": {
        "id": "Hyq686EdAzf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input layer\n",
        "input_layer = Input(shape=(28, 28))\n",
        "\n",
        "# Flatten the input\n",
        "flatten_layer = keras.layers.Flatten()(input_layer)\n",
        "\n",
        "# Define the wide branch with modified activation function and weight initialization\n",
        "wide_branch = Dense(128, activation='tanh', kernel_initializer='glorot_uniform')(flatten_layer)\n",
        "wide_branch = Dense(64, activation='tanh', kernel_initializer='glorot_uniform')(wide_branch)\n",
        "\n",
        "# Define the deep branch with modified activation function and weight initialization\n",
        "deep_branch = Dense(128, activation='sigmoid', kernel_initializer='he_uniform')(flatten_layer)\n",
        "deep_branch = Dense(128, activation='sigmoid', kernel_initializer='he_uniform')(deep_branch)\n",
        "deep_branch = Dense(64, activation='sigmoid', kernel_initializer='he_uniform')(deep_branch)\n",
        "\n",
        "# Concatenate the wide and deep branches\n",
        "concat_layer = Concatenate()([wide_branch, deep_branch])\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(10, activation='softmax')(concat_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "EuuyEW13_dWi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "5Kv0fDX1BWAT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt3zRe2FBaZT",
        "outputId": "cba6cfa4-c328-415a-c706-29c5f2f3feeb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8845\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94025, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 11ms/step - loss: 0.4228 - accuracy: 0.8847 - val_loss: 0.2163 - val_accuracy: 0.9402\n",
            "Epoch 2/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9460\n",
            "Epoch 2: val_accuracy improved from 0.94025 to 0.95517, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.1848 - accuracy: 0.9461 - val_loss: 0.1576 - val_accuracy: 0.9552\n",
            "Epoch 3/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9617\n",
            "Epoch 3: val_accuracy improved from 0.95517 to 0.96308, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.1306 - accuracy: 0.9616 - val_loss: 0.1287 - val_accuracy: 0.9631\n",
            "Epoch 4/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9715\n",
            "Epoch 4: val_accuracy improved from 0.96308 to 0.96467, saving model to best_model.h5\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0988 - accuracy: 0.9715 - val_loss: 0.1155 - val_accuracy: 0.9647\n",
            "Epoch 5/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9780\n",
            "Epoch 5: val_accuracy improved from 0.96467 to 0.97100, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0761 - accuracy: 0.9780 - val_loss: 0.1008 - val_accuracy: 0.9710\n",
            "Epoch 6/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9823\n",
            "Epoch 6: val_accuracy did not improve from 0.97100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0609 - accuracy: 0.9823 - val_loss: 0.0998 - val_accuracy: 0.9703\n",
            "Epoch 7/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9863\n",
            "Epoch 7: val_accuracy improved from 0.97100 to 0.97175, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0474 - accuracy: 0.9863 - val_loss: 0.0958 - val_accuracy: 0.9718\n",
            "Epoch 8/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9891\n",
            "Epoch 8: val_accuracy improved from 0.97175 to 0.97458, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0392 - accuracy: 0.9891 - val_loss: 0.0907 - val_accuracy: 0.9746\n",
            "Epoch 9/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9915\n",
            "Epoch 9: val_accuracy did not improve from 0.97458\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0302 - accuracy: 0.9915 - val_loss: 0.0894 - val_accuracy: 0.9746\n",
            "Epoch 10/20\n",
            "369/375 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9940\n",
            "Epoch 10: val_accuracy did not improve from 0.97458\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0891 - val_accuracy: 0.9732\n",
            "Epoch 11/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9961\n",
            "Epoch 11: val_accuracy improved from 0.97458 to 0.97492, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.0173 - accuracy: 0.9961 - val_loss: 0.0893 - val_accuracy: 0.9749\n",
            "Epoch 12/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9970\n",
            "Epoch 12: val_accuracy did not improve from 0.97492\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0137 - accuracy: 0.9970 - val_loss: 0.0951 - val_accuracy: 0.9733\n",
            "Epoch 13/20\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9978\n",
            "Epoch 13: val_accuracy did not improve from 0.97492\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 0.0951 - val_accuracy: 0.9746\n",
            "Epoch 13: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model saved during training\n",
        "best_model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test Accuracy:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTFcc7ueBdsV",
        "outputId": "3632715a-9432-4399-8d41-a0fdba0921e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0781 - accuracy: 0.9763\n",
            "Test Accuracy: 0.9763000011444092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test data\n",
        "predictions = best_model.predict(x_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "true_labels = tf.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = tf.keras.metrics.Precision()(true_labels, predicted_labels)\n",
        "recall = tf.keras.metrics.Recall()(true_labels, predicted_labels)\n",
        "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Precision:', precision.numpy())\n",
        "print('Recall:', recall.numpy())\n",
        "print('F1 Score:', f1_score.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvrbJShB7gy",
        "outputId": "b422b6b2-555a-45d3-d6d1-3fc16e97adfd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "Precision: 0.9984481\n",
            "Recall: 0.99855876\n",
            "F1 Score: 0.9985034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "游릭 Question 3\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAvlJY1uEGek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**let's experiment with different optimizers. We'll modify the optimizer used in the model and compile it before training.**\n",
        "\n"
      ],
      "metadata": {
        "id": "bU7pdH7wEtKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6RXPgMnZGSbb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different optimizers\n",
        "optimizers = ['sgd', 'rmsprop', 'adam', 'adagrad', 'adadelta', 'adamax', 'nadam']\n",
        "Test_Accuracy=[];\n",
        "\n",
        "for optimizer_name in optimizers:\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    \n",
        "    # Compile the model with the current optimizer\n",
        "    optimizer = keras.optimizers.get(optimizer_name)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # Train the model\n",
        "    print(f\"Training model with optimizer: {optimizer_name}\")\n",
        "    history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "    \n",
        "    # Load the best model saved during training\n",
        "    best_model = keras.models.load_model('best_model.h5')\n",
        "    \n",
        "    # Evaluate the model on the test data\n",
        "    test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "    \n",
        "    # store n Print the test accuracy\n",
        "    Test_Accuracy.append(test_accuracy)\n",
        "    print('Test Accuracy:', test_accuracy)\n",
        "    print('------------------------------------')\n",
        "print(\"Test Accuracies for Different Optimizers:\")\n",
        "for i in range(len(optimizers)):\n",
        "    print(f\"{optimizers[i]}: {Test_Accuracy[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e9ieZmBCP2T",
        "outputId": "e9839c27-46ec-4016-f7a4-cb159395f6be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with optimizer: sgd\n",
            "Epoch 1/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 9.2262e-05 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy improved from -inf to 0.97775, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 9.2070e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 9.1936e-05 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 9.1936e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 3/20\n",
            "369/375 [============================>.] - ETA: 0s - loss: 9.1475e-05 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 9.1811e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 4/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 9.1693e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 9.1693e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 5/20\n",
            "369/375 [============================>.] - ETA: 0s - loss: 9.1279e-05 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 9.1583e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 6/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 9.1479e-05 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 9.1479e-05 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9778\n",
            "Epoch 6: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0979 - accuracy: 0.9796\n",
            "Test Accuracy: 0.9796000123023987\n",
            "------------------------------------\n",
            "Training model with optimizer: rmsprop\n",
            "Epoch 1/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 9.2816e-05 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 0.97775\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 9.2675e-05 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9777\n",
            "Epoch 2/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 8.5159e-05 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy improved from 0.97775 to 0.97800, saving model to best_model.h5\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 8.5289e-05 - accuracy: 1.0000 - val_loss: 0.1212 - val_accuracy: 0.9780\n",
            "Epoch 3/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 7.9415e-05 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 7.9415e-05 - accuracy: 1.0000 - val_loss: 0.1211 - val_accuracy: 0.9777\n",
            "Epoch 4/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 7.4199e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 7.4157e-05 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9776\n",
            "Epoch 4: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0988 - accuracy: 0.9800\n",
            "Test Accuracy: 0.9800000190734863\n",
            "------------------------------------\n",
            "Training model with optimizer: adam\n",
            "Epoch 1/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982\n",
            "Epoch 1: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 5s 11ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.1358 - val_accuracy: 0.9756\n",
            "Epoch 2/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9988\n",
            "Epoch 2: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.1420 - val_accuracy: 0.9732\n",
            "Epoch 3/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
            "Epoch 3: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1260 - val_accuracy: 0.9768\n",
            "Epoch 4/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 1.6228e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97800\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 1.6251e-04 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9776\n",
            "Epoch 5/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 1.0401e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.97800 to 0.97858, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 1.0401e-04 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9786\n",
            "Epoch 6/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 8.5830e-05 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 8.5830e-05 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9783\n",
            "Epoch 7/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 7.4494e-05 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 7.4343e-05 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9781\n",
            "Epoch 8/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 6.4692e-05 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 6.4935e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 8: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1028 - accuracy: 0.9785\n",
            "Test Accuracy: 0.9785000085830688\n",
            "------------------------------------\n",
            "Training model with optimizer: adagrad\n",
            "Epoch 1/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 5.6991e-05 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 5.6945e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 5.6932e-05 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 5.6932e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 3/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 5.6942e-05 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 5.6918e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 4/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 5.6906e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 5.6906e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 4: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1028 - accuracy: 0.9785\n",
            "Test Accuracy: 0.9785000085830688\n",
            "------------------------------------\n",
            "Training model with optimizer: adadelta\n",
            "Epoch 1/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 5.6850e-05 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 7s 14ms/step - loss: 5.6895e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 2/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 5.6869e-05 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 5.6891e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 3/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 5.6997e-05 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 5.6887e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 4/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 5.6772e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 5.6884e-05 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9782\n",
            "Epoch 4: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1028 - accuracy: 0.9785\n",
            "Test Accuracy: 0.9785000085830688\n",
            "------------------------------------\n",
            "Training model with optimizer: adamax\n",
            "Epoch 1/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 7.3609e-05 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 10ms/step - loss: 7.3516e-05 - accuracy: 1.0000 - val_loss: 0.1276 - val_accuracy: 0.9782\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 4.3536e-05 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 4.3536e-05 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9781\n",
            "Epoch 3/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 3.6080e-05 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 3.6139e-05 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 0.9783\n",
            "Epoch 4/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 2.9581e-05 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 2.9555e-05 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9780\n",
            "Epoch 4: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1028 - accuracy: 0.9785\n",
            "Test Accuracy: 0.9785000085830688\n",
            "------------------------------------\n",
            "Training model with optimizer: nadam\n",
            "Epoch 1/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9989\n",
            "Epoch 1: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 7s 12ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.1502 - val_accuracy: 0.9758\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n",
            "Epoch 2: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.1554 - val_accuracy: 0.9753\n",
            "Epoch 3/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9972\n",
            "Epoch 3: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 0.1524 - val_accuracy: 0.9737\n",
            "Epoch 4/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 4: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.1459 - val_accuracy: 0.9766\n",
            "Epoch 5/20\n",
            "370/375 [============================>.] - ETA: 0s - loss: 3.4122e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 3.4572e-04 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 0.9780\n",
            "Epoch 6/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 6: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.1531 - val_accuracy: 0.9735\n",
            "Epoch 7/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9989\n",
            "Epoch 7: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.1500 - val_accuracy: 0.9747\n",
            "Epoch 8/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 8.3748e-04 - accuracy: 0.9998\n",
            "Epoch 8: val_accuracy did not improve from 0.97858\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 8.3776e-04 - accuracy: 0.9998 - val_loss: 0.1414 - val_accuracy: 0.9767\n",
            "Epoch 8: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1028 - accuracy: 0.9785\n",
            "Test Accuracy: 0.9785000085830688\n",
            "------------------------------------\n",
            "Test Accuracies for Different Optimizers:\n",
            "sgd: 0.9796000123023987\n",
            "rmsprop: 0.9800000190734863\n",
            "adam: 0.9785000085830688\n",
            "adagrad: 0.9785000085830688\n",
            "adadelta: 0.9785000085830688\n",
            "adamax: 0.9785000085830688\n",
            "nadam: 0.9785000085830688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "游릭 Question 3:\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCy9HjfNItLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This time implement different regularization methods such as L1, L2, and Dropout to reduce overfitting.**"
      ],
      "metadata": {
        "id": "fabnbhSGI5ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(28, 28))\n",
        "\n",
        "# Flatten the input\n",
        "flatten_layer = keras.layers.Flatten()(input_layer)\n",
        "\n",
        "# Define the wide branch with L2 regularization\n",
        "wide_branch = Dense(128, activation='tanh', kernel_regularizer=l2(0.01))(flatten_layer)\n",
        "wide_branch = Dropout(0.5)(wide_branch)\n",
        "wide_branch = Dense(64, activation='tanh', kernel_regularizer=l2(0.01))(wide_branch)\n",
        "wide_branch = Dropout(0.5)(wide_branch)\n",
        "\n",
        "# Define the deep branch with L1 regularization\n",
        "deep_branch = Dense(128, activation='sigmoid', kernel_regularizer=l1(0.01))(flatten_layer)\n",
        "deep_branch = Dropout(0.5)(deep_branch)\n",
        "deep_branch = Dense(128, activation='sigmoid', kernel_regularizer=l1(0.01))(deep_branch)\n",
        "deep_branch = Dropout(0.5)(deep_branch)\n",
        "deep_branch = Dense(64, activation='sigmoid', kernel_regularizer=l1(0.01))(deep_branch)\n",
        "deep_branch = Dropout(0.5)(deep_branch)\n",
        "\n",
        "# Concatenate the wide and deep branches\n",
        "concat_layer = Concatenate()([wide_branch, deep_branch])\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(10, activation='softmax')(concat_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Set up early stopping and model checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Load the best model saved during training\n",
        "best_model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSSGLvk7FME9",
        "outputId": "5fdc0e6a-2a28-462b-d7ec-eb6e9ec4ac51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 7.8904 - accuracy: 0.7650\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90767, saving model to best_model.h5\n",
            "375/375 [==============================] - 6s 13ms/step - loss: 7.8172 - accuracy: 0.7662 - val_loss: 0.8511 - val_accuracy: 0.9077\n",
            "Epoch 2/20\n",
            "373/375 [============================>.] - ETA: 0s - loss: 0.9265 - accuracy: 0.8543\n",
            "Epoch 2: val_accuracy did not improve from 0.90767\n",
            "375/375 [==============================] - 6s 16ms/step - loss: 0.9258 - accuracy: 0.8545 - val_loss: 0.7427 - val_accuracy: 0.8974\n",
            "Epoch 3/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.8528 - accuracy: 0.8632\n",
            "Epoch 3: val_accuracy did not improve from 0.90767\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.8528 - accuracy: 0.8629 - val_loss: 0.7015 - val_accuracy: 0.9050\n",
            "Epoch 4/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.8282 - accuracy: 0.8647\n",
            "Epoch 4: val_accuracy improved from 0.90767 to 0.90883, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.8284 - accuracy: 0.8647 - val_loss: 0.6844 - val_accuracy: 0.9088\n",
            "Epoch 5/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.8116 - accuracy: 0.8674\n",
            "Epoch 5: val_accuracy did not improve from 0.90883\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.8118 - accuracy: 0.8674 - val_loss: 0.6683 - val_accuracy: 0.9078\n",
            "Epoch 6/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.8013 - accuracy: 0.8693\n",
            "Epoch 6: val_accuracy did not improve from 0.90883\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.8012 - accuracy: 0.8694 - val_loss: 0.6720 - val_accuracy: 0.9015\n",
            "Epoch 7/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.7941 - accuracy: 0.8679\n",
            "Epoch 7: val_accuracy did not improve from 0.90883\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.7940 - accuracy: 0.8680 - val_loss: 0.6577 - val_accuracy: 0.9072\n",
            "Epoch 8/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.7839 - accuracy: 0.8701\n",
            "Epoch 8: val_accuracy did not improve from 0.90883\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.7839 - accuracy: 0.8700 - val_loss: 0.6533 - val_accuracy: 0.9078\n",
            "Epoch 9/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.7825 - accuracy: 0.8691\n",
            "Epoch 9: val_accuracy improved from 0.90883 to 0.90942, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.7828 - accuracy: 0.8691 - val_loss: 0.6414 - val_accuracy: 0.9094\n",
            "Epoch 10/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.7755 - accuracy: 0.8687\n",
            "Epoch 10: val_accuracy did not improve from 0.90942\n",
            "375/375 [==============================] - 5s 15ms/step - loss: 0.7753 - accuracy: 0.8688 - val_loss: 0.6441 - val_accuracy: 0.9068\n",
            "Epoch 11/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.7720 - accuracy: 0.8694\n",
            "Epoch 11: val_accuracy improved from 0.90942 to 0.91217, saving model to best_model.h5\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.7723 - accuracy: 0.8693 - val_loss: 0.6352 - val_accuracy: 0.9122\n",
            "Epoch 12/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.7713 - accuracy: 0.8703\n",
            "Epoch 12: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.7714 - accuracy: 0.8701 - val_loss: 0.6304 - val_accuracy: 0.9082\n",
            "Epoch 13/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.7660 - accuracy: 0.8711\n",
            "Epoch 13: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.7660 - accuracy: 0.8711 - val_loss: 0.6282 - val_accuracy: 0.9087\n",
            "Epoch 14/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.8700\n",
            "Epoch 14: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.7604 - accuracy: 0.8700 - val_loss: 0.6411 - val_accuracy: 0.9029\n",
            "Epoch 15/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.7628 - accuracy: 0.8722\n",
            "Epoch 15: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.7628 - accuracy: 0.8722 - val_loss: 0.6259 - val_accuracy: 0.9107\n",
            "Epoch 16/20\n",
            "371/375 [============================>.] - ETA: 0s - loss: 0.7554 - accuracy: 0.8719\n",
            "Epoch 16: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.7560 - accuracy: 0.8715 - val_loss: 0.6436 - val_accuracy: 0.9053\n",
            "Epoch 17/20\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.7591 - accuracy: 0.8702\n",
            "Epoch 17: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 5s 12ms/step - loss: 0.7590 - accuracy: 0.8703 - val_loss: 0.6285 - val_accuracy: 0.9054\n",
            "Epoch 18/20\n",
            "370/375 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.8717\n",
            "Epoch 18: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.7569 - accuracy: 0.8719 - val_loss: 0.6177 - val_accuracy: 0.9118\n",
            "Epoch 19/20\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.7587 - accuracy: 0.8705\n",
            "Epoch 19: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 0.7587 - accuracy: 0.8705 - val_loss: 0.6219 - val_accuracy: 0.9080\n",
            "Epoch 20/20\n",
            "372/375 [============================>.] - ETA: 0s - loss: 0.7563 - accuracy: 0.8696\n",
            "Epoch 20: val_accuracy did not improve from 0.91217\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 0.7558 - accuracy: 0.8699 - val_loss: 0.6291 - val_accuracy: 0.9084\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.9104\n",
            "Test Accuracy: 0.9103999733924866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "游릭 Question 4:\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "054bt9XgLJhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing ResNet-34 architecture from the scratch using Keras Sequential API. then train the network to predict on MNIST Fashion dataset. Evaluate model using appropriate metrics.**"
      ],
      "metadata": {
        "id": "SOi6CPSjLOlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Add, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Expand dimensions for grayscale images\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the Residual Block\n",
        "def residual_block(x, filters, strides=1):\n",
        "    shortcut = x\n",
        "    \n",
        "    # First convolutional layer\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # Second convolutional layer\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    # If the number of filters changes or the strides are greater than 1, apply a convolution to shortcut path\n",
        "    if strides > 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    \n",
        "    # Add the shortcut to the main path\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Build the ResNet-34 model\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(64, kernel_size=(7, 7), strides=2, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
        "\n",
        "x = residual_block(x, filters=64)\n",
        "x = residual_block(x, filters=64)\n",
        "x = residual_block(x, filters=64)\n",
        "\n",
        "x = residual_block(x, filters=128, strides=2)\n",
        "x = residual_block(x, filters=128)\n",
        "x = residual_block(x, filters=128)\n",
        "x = residual_block(x, filters=128)\n",
        "\n",
        "x = residual_block(x, filters=256, strides=2)\n",
        "x = residual_block(x, filters=256)\n",
        "x = residual_block(x, filters=256)\n",
        "x = residual_block(x, filters=256)\n",
        "x = residual_block(x, filters=256)\n",
        "x = residual_block(x, filters=256)\n",
        "\n",
        "x = residual_block(x, filters=512, strides=2)\n",
        "x = residual_block(x, filters=512)\n",
        "x = residual_block(x, filters=512)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Flatten()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping and model checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Load the best model saved during training\n",
        "best_model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print('Test Accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaotFjXFJMES",
        "outputId": "72b5db64-c9c8-4ed7-cb1d-d6210f5fbbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "106/375 [=======>......................] - ETA: 41:25 - loss: 0.6881 - accuracy: 0.7544"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiDh6VpJMKR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}